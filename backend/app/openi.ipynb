{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8954582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "import os\n",
    "# import os\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "\n",
    "def llm_refine(query):\n",
    "    # reranked_texts = [node.node.get_content() for node in ranked_nodes]\n",
    "    # context = \"\\n\\n\".join(reranked_texts)\n",
    "    \n",
    "    \n",
    "    # Load Azure credentials from environment variables\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    azure_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "\n",
    "\n",
    "# # Load Azure credentials from environment variables\n",
    "# azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://my-azure-openai.openai.azure.com/\")\n",
    "# azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"abcd1234...\")\n",
    "# azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt4-mini-deployment\")\n",
    "# azure_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\")\n",
    "\n",
    "# Initialize the AzureOpenAI LLM\n",
    "    llm = AzureOpenAI(\n",
    "        model=azure_deployment,          # often same as deployment_name for Azure\n",
    "        engine=azure_deployment,\n",
    "        api_key=azure_api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=azure_api_version\n",
    "    )\n",
    "\n",
    "    # history_text = \"\"\n",
    "    # if chat_history:\n",
    "    #     history_text = \"\\n\".join([f\"{t['role']}: {t['content']}\" for t in chat_history])\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use the conversation history and provided context\n",
    "    to answer the user query.\n",
    "\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example: Ask a question\n",
    "    # prompt = \"Explain quantum computing in very simple terms.\"\n",
    "    # response = llm.complete(prompt)\n",
    "        \n",
    "    # print(\"Response:\", response.text)\n",
    "\n",
    "\n",
    "    response = llm.complete(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = \"Explain quantum computing in very simple terms.\"\n",
    "g = llm_refine(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd2c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "# def test_azure_llm():\n",
    "\n",
    "\n",
    "    # üëá Use the deployment name you created in Azure portal\n",
    "    deployment_name = \"gpt-4-1-mini\"  \n",
    "\n",
    "    if not api_key or not endpoint or not api_version:\n",
    "        raise ValueError(\"‚ùå Missing one or more Azure env vars (API key, endpoint, or version).\")\n",
    "\n",
    "    # Initialize LlamaIndex's Azure LLM wrapper\n",
    "    llm = AzureOpenAI(\n",
    "        model=\"gpt-4.1-mini\",      # model family\n",
    "        engine=deployment_name,    # your Azure deployment name\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    )\n",
    "\n",
    "    # Simple test prompt\n",
    "    response = llm.complete(\"Hello! Can you explain what 2+2 is?\")\n",
    "    print(\"‚úÖ Azure LLM Response:\", response.text)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "test_azure_llm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f6d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = \"gpt-4-1-mini\"  \n",
    "\n",
    "# Initialize LlamaIndex's Azure LLM wrapper\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4.1-mini\",      # model family\n",
    "    engine=deployment_name,    # your Azure deployment name\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "# Simple test prompt\n",
    "response = llm.complete(\"Hello! Can you explain what 2+2 is?\")\n",
    "print(\"‚úÖ Azure LLM Response:\", response.text)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# test_azure_llm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4893b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()  # load .env file if present\n",
    "\n",
    "def test_azure_llm():\n",
    "    \n",
    "    # deployment_name = \"gpt-4-1-mini\"  \n",
    "    # api_key = \"F0oZQudTzsI1pfMzEuPN4He7Cg0SpWhqJkDMq6QRRteTC0UENmFgJQQJ99BHACYeBjFXJ3w3AAABACOGOrqB\"\n",
    "    # endpoint =\"https://praval-ds.openai.azure.com/\"\n",
    "    # api_version = \"2024-12-01-preview\"\n",
    "    # model=\"gpt-4.1-mini\",      # model family\n",
    "    # engine=deployment_name,    # your Azure deployment name\n",
    "    # api_key=\n",
    "    # azure_endpoint=,\n",
    "    # api_version=,\n",
    "\n",
    "    deployment_name = \"gpt-4.1-mini\"  # ‚ö†Ô∏è must match Azure Portal deployment\n",
    "\n",
    "    llm = AzureOpenAI(\n",
    "        model=\"gpt-4.1-mini\",    # family name\n",
    "        engine=deployment_name,  # your deployment name\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    )\n",
    "\n",
    "    response = llm.complete(\"Hello, can you confirm you're working?\")\n",
    "    print(\"‚úÖ Azure LLM Response:\", response.text)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1245b1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure LLM Response: Hello! Yes, I'm here and ready to help. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "test_azure_llm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nexora_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
